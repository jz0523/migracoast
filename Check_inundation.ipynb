{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a9bcb6-1a85-40da-9359-1b62682649e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from rasterio.enums import Resampling\n",
    "from pyproj import Transformer\n",
    "import rasterio\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "from collections import defaultdict\n",
    "from flask import Flask, request, jsonify\n",
    "import io\n",
    "import base64\n",
    "import statistics\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5f96736-c9f4-4b31-bd47-579be88c2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_ar6_ssp585 = 'ar6data/total_ssp585_medium_confidence_values.nc'\n",
    "\n",
    "file_path_ar6_ssp245 = 'ar6data/total_ssp245_medium_confidence_values.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae02813b-8470-4964-97a2-b9c2f39d749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_and_flood_csv = \"cmip6data/StormEvents_details-ftp_v1.0_d2000_c20220425.csv\"\n",
    "# Define paths to the inundation geotiffs\n",
    "inundation_maps_MA = {\n",
    "    0: \"inundationmaps/MA_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/MA_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/MA_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/MA_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/MA_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/MA_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/MA_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/MA_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/MA_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_PA = {\n",
    "    0: \"inundationmaps/PA_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/PA_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/PA_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/PA_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/PA_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/PA_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/PA_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/PA_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/PA_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_RI = {\n",
    "    0: \"inundationmaps/RI_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/RI_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/RI_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/RI_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/RI_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/RI_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/RI_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/RI_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/RI_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_AL = {\n",
    "    0: \"inundationmaps/AL_connectRaster_0.tif\",\n",
    "    1: \"inundationmaps/AL_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/AL_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/AL_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/AL_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/AL_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/AL_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/AL_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_CT = {\n",
    "    0: \"inundationmaps/CT_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/CT_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/CT_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/CT_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/CT_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/CT_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/CT_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/CT_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/CT_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_DC = {\n",
    "    0: \"inundationmaps/DC_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/DC_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/DC_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/DC_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/DC_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/DC_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/DC_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/DC_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/DC_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_DE = {\n",
    "    0: \"inundationmaps/DE_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/DE_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/DE_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/DE_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/DE_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/DE_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/DE_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/DE_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/DE_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_MS = {\n",
    "    0: \"inundationmaps/MS_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/MS_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/MS_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/MS_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/MS_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/MS_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/MS_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/MS_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/MS_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_NH = {\n",
    "    0: \"inundationmaps/NH_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/NH_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/NH_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/NH_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/NH_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/NH_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/NH_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/NH_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/NH_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_FL = {\n",
    "    0: \"inundationmaps/FL_SE_connectRaster_0_0.tif\",\n",
    "    0.5: \"inundationmaps/FL_SE_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/FL_SE_connectRaster_1_0.tif\",\n",
    "    1.5: \"inundationmaps/FL_SE_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/FL_SE_connectRaster_2_0.tif\",\n",
    "    2.5: \"inundationmaps/FL_SE_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/FL_SE_connectRaster_3_0.tif\",\n",
    "    3.5: \"inundationmaps/FL_SE_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/FL_SE_connectRaster_4_0.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_CA = {\n",
    "    0: \"inundationmaps/CA_LOX_connectRaster_0.tif\",\n",
    "    1: \"inundationmaps/CA_LOX_connectRaster_1.tif\",\n",
    "    2: \"inundationmaps/CA_LOX_connectRaster_2.tif\",\n",
    "    3: \"inundationmaps/CA_LOX_connectRaster_3.tif\",\n",
    "    4: \"inundationmaps/CA_LOX_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "inundation_maps_NY = {\n",
    "    0: \"inundationmaps/NY_M_connectRaster_0.tif\",\n",
    "    0.5: \"inundationmaps/NY_M_connectRaster_0_5.tif\",\n",
    "    1: \"inundationmaps/NY_M_connectRaster_1.tif\",\n",
    "    1.5: \"inundationmaps/NY_M_connectRaster_1_5.tif\",\n",
    "    2: \"inundationmaps/NY_M_connectRaster_2.tif\",\n",
    "    2.5: \"inundationmaps/NY_M_connectRaster_2_5.tif\",\n",
    "    3: \"inundationmaps/NY_M_connectRaster_3.tif\",\n",
    "    3.5: \"inundationmaps/NY_M_connectRaster_3_5.tif\",\n",
    "    4: \"inundationmaps/NY_M_connectRaster_4.tif\"\n",
    "}\n",
    "\n",
    "state_inundation_maps = {\n",
    "    \"Massachusetts\": inundation_maps_MA,\n",
    "    \"MA\": inundation_maps_MA,\n",
    "    \"Pennsylvania\": inundation_maps_PA,\n",
    "    \"PA\": inundation_maps_PA,\n",
    "    \"Rhode Island\": inundation_maps_RI,\n",
    "    \"RI\": inundation_maps_RI,\n",
    "   \"Alabama\": inundation_maps_AL,\n",
    "   \"AL\": inundation_maps_AL,\n",
    "    \"Connecticut\": inundation_maps_CT,\n",
    "    \"CT\": inundation_maps_CT,\n",
    "    \"Washington, DC\": inundation_maps_DC,\n",
    "    \"DC\": inundation_maps_DC,\n",
    "    \"Delaware\": inundation_maps_DE,\n",
    "    \"DE\": inundation_maps_DE,\n",
    "    \"Mississippi\": inundation_maps_MS,\n",
    "    \"MS\": inundation_maps_MS,\n",
    "    \"New Hampshire\": inundation_maps_NH,\n",
    "    \"NH\": inundation_maps_NH,\n",
    "    \"FL\": inundation_maps_FL,\n",
    "    \"Florida\": inundation_maps_FL,\n",
    "    \"California\": inundation_maps_CA,\n",
    "    \"CA\": inundation_maps_CA,\n",
    "    \"New York\": inundation_maps_NY,\n",
    "    \"NY\": inundation_maps_NY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30029950-88c5-4625-bf24-40e0d5ecafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sea_level_projection(input_lat, input_lon, file_path):\n",
    "    # Load the dataset\n",
    "    ds = nc.Dataset(file_path, 'r')\n",
    "    \n",
    "    # Extract latitude, longitude, sea level change, and years\n",
    "    latitude = ds.variables['lat'][:]\n",
    "    longitude = ds.variables['lon'][:]\n",
    "    sea_level_change = ds.variables['sea_level_change'][:]  # (quantiles, years, locations)\n",
    "    years = ds.variables['years'][:]\n",
    "    \n",
    "    # Adjust longitude to [-180, 180] if necessary\n",
    "    if np.max(longitude) > 180:\n",
    "        longitude = np.where(longitude > 180, longitude - 360, longitude)\n",
    "    \n",
    "    # Function to find nearby points within a given threshold\n",
    "    def find_nearby_points(lat_threshold, lon_threshold, max_points=5):\n",
    "        nearby_lat = np.argwhere(np.abs(latitude - input_lat) < lat_threshold).flatten()\n",
    "        nearby_lon = np.argwhere(np.abs(longitude[nearby_lat] - input_lon) < lon_threshold).flatten()\n",
    "        qualified_idx = nearby_lat[nearby_lon]\n",
    "        return qualified_idx[:max_points]  # Return at most max_points\n",
    "    \n",
    "    # Try finding points within 0.5 degrees\n",
    "    qualified_idx = find_nearby_points(0.5, 0.5)\n",
    "    \n",
    "    # If no points found, expand the search to 1 degree\n",
    "    if len(qualified_idx) == 0:\n",
    "        qualified_idx = find_nearby_points(1.0, 1.0)\n",
    "    \n",
    "    # If still no points found, return an empty DataFrame\n",
    "    if len(qualified_idx) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Extract sea level change data for the nearby coordinates\n",
    "    # Assuming quantile 50 (median) for sea level rise projections\n",
    "    quantile_idx = 50  # Adjust if necessary\n",
    "\n",
    "    sea_level_rise_data = []\n",
    "    for idx in qualified_idx:\n",
    "        slr_at_location = sea_level_change[quantile_idx, :, idx]  # Sea level rise at this location for all years\n",
    "        sea_level_rise_data.append(slr_at_location)\n",
    "\n",
    "    # Step 3: Average sea level rise over all nearby locations\n",
    "    sea_level_rise_avg = np.mean(sea_level_rise_data, axis=0)\n",
    "\n",
    "    # Create a DataFrame for the time series\n",
    "    df = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Sea Level Rise': sea_level_rise_avg\n",
    "    })\n",
    "    df = df[df['Year'] <= 2100]\n",
    "    # Close the dataset\n",
    "    ds.close()\n",
    "    if not np.ma.is_masked(df):\n",
    "        return df\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "162fb878-6add-4396-9543-4e5c192a1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_boundary(inundation_map):\n",
    "    with rasterio.open(inundation_map) as src:\n",
    "            # Check the bounds of the raster file (lon_min, lon_max should be in [-180, 180])\n",
    "            lat_min, lat_max = src.bounds.bottom, src.bounds.top\n",
    "            lon_min, lon_max = src.bounds.left, src.bounds.right\n",
    "    return lat_min, lat_max, lon_min, lon_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b7bd666-6d2e-4449-8d07-0d51ca99097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a coordinate is inundated using a smaller window\n",
    "def check_inundation(file_path, lat, lon, window_size=5):\n",
    "    def check_point(lat, lon):\n",
    "        \"\"\"Helper function to check inundation for a single point.\"\"\"\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Check the bounds of the raster file (lon_min, lon_max should be in [-180, 180])\n",
    "            lat_min, lat_max = src.bounds.bottom, src.bounds.top\n",
    "            lon_min, lon_max = src.bounds.left, src.bounds.right\n",
    "            \n",
    "            # Transform the input coordinates to the raster's CRS\n",
    "            transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n",
    "            x, y = transformer.transform(lon, lat)\n",
    "            \n",
    "            # Check if the input coordinate is within the bounds of the raster\n",
    "            if not (lon_min <= x <= lon_max and lat_min <= y <= lat_max):\n",
    "                return \"OUTSIDE BOUNDARY\"\n",
    "            \n",
    "            # Get the row and column indices for the input coordinate\n",
    "            row, col = src.index(x, y)\n",
    "            \n",
    "            # Define a window around the input coordinate\n",
    "            row_start = max(row - window_size, 0)\n",
    "            row_end = min(row + window_size, src.height)\n",
    "            col_start = max(col - window_size, 0)\n",
    "            col_end = min(col + window_size, src.width)\n",
    "            \n",
    "            window = rasterio.windows.Window(col_start, row_start, col_end - col_start, row_end - row_start)\n",
    "            \n",
    "            # Read only the window of interest and ensure it's valid\n",
    "            try:\n",
    "                data = src.read(1, window=window, masked=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading data in the window: {e}\")\n",
    "                return \"OUTSIDE BOUNDARY\"\n",
    "\n",
    "            # Calculate local row and column within the window\n",
    "            local_row = row - row_start\n",
    "            local_col = col - col_start\n",
    "\n",
    "            # Ensure the indices are within bounds of the data window\n",
    "            if local_row >= data.shape[0] or local_col >= data.shape[1]:\n",
    "                return \"OUTSIDE BOUNDARY\"\n",
    "            \n",
    "            # Check the value at the location and handle NoData explicitly\n",
    "            value_at_location = data[local_row, local_col]\n",
    "\n",
    "            # Handle NoData and inundation values explicitly\n",
    "            if value_at_location == 0 or value_at_location == 0.0:\n",
    "                return False  # Uninundated\n",
    "            elif value_at_location == 1 or value_at_location == 1.0:\n",
    "                return True  # Inundated\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    # First check the original input point\n",
    "    result = check_point(lat, lon)\n",
    "    if result in [True, False]:  # If we get a valid result (inundated or not), return it\n",
    "        #print(\"The value is taken directly from the input coordinate (not from surrounding buffer)\")\n",
    "        return result\n",
    "    elif result == \"OUTSIDE BOUNDARY\":\n",
    "        return \"OUTSIDE BOUNDARY\"\n",
    "\n",
    "    # If the original point has no value, check surrounding 8 points\n",
    "    lat_offset = 0.0001\n",
    "    lon_offset = 0.0001\n",
    "    nearby_points = [\n",
    "        (lat + lat_offset, lon),\n",
    "        (lat - lat_offset, lon),\n",
    "        (lat, lon + lon_offset),\n",
    "        (lat, lon - lon_offset),\n",
    "        (lat + lat_offset, lon + lon_offset),\n",
    "        (lat - lat_offset, lon - lon_offset),\n",
    "        (lat + lat_offset, lon - lon_offset),\n",
    "        (lat - lat_offset, lon + lon_offset),\n",
    "    ]\n",
    "    nearby_results = []\n",
    "    for nearby_lat, nearby_lon in nearby_points:\n",
    "        nearby_result = check_point(nearby_lat, nearby_lon)\n",
    "        if nearby_result == True:  # Prioritize inundation\n",
    "            #print(\"The value is taken from surrounding buffer\")\n",
    "            return True\n",
    "        elif nearby_result == False:  # Uninundated\n",
    "            #print(\"The value is taken from surrounding buffer\")\n",
    "            nearby_results.append(nearby_result)\n",
    "            continue  # Keep checking for inundation\n",
    "        # Skip if no value (None or \"UNKNOWN_VALUE or NO_DATA\")\n",
    "    if nearby_results:\n",
    "        return False\n",
    "    # If none of the points had a value, return no value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c46be67-9640-4bf2-b8d0-95d64906186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the geolocator with a more specific user agent\n",
    "geolocator = Nominatim(user_agent=\"my_geocoding_application\")\n",
    "\n",
    "def get_place_name(lat, lon):\n",
    "    try:\n",
    "        # Introduce a delay to avoid rate limiting\n",
    "        time.sleep(0.5)  # Wait for 1 second between requests\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "        if location and location.address:\n",
    "            address = location.raw['address']\n",
    "            # Return the first available place name: city, town, or village\n",
    "            if 'city' in address:\n",
    "                return address['city']\n",
    "            elif 'town' in address:\n",
    "                return address['town']\n",
    "            elif 'village' in address:\n",
    "                return address['village']\n",
    "            else:\n",
    "                return \"Unknown\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f621a1a1-0591-4d04-8dee-38e397f9cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_uninundated_locations(input_lat, input_lon, inundation_map, search_radius=0.08, step_size=0.02, max_locations=3, safe_margin=0.005, safe_step=0.0025):\n",
    "    # Get the boundary of the map (latitude and longitude limits)\n",
    "    \n",
    "    lat_min, lat_max, lon_min, lon_max = get_map_boundary(inundation_map)\n",
    "    if not (lon_min <= input_lon <= lon_max and lat_min <= input_lat <= lat_max):\n",
    "        return None\n",
    "    # List to store safe locations\n",
    "    safe_locations = []\n",
    "    unique_place_names = set()  # Set to avoid duplicate place names\n",
    "\n",
    "    layer = 1  # Start at layer 1 (surrounding the input point)\n",
    "    \n",
    "    # Continue searching until the required number of safe locations is found or search_radius is exceeded\n",
    "    while len(safe_locations) < max_locations and layer * step_size <= search_radius:\n",
    "        # Generate the nearby points for the current layer\n",
    "        nearby_points = []\n",
    "        \n",
    "        # Top row (moving right)\n",
    "        for i in range(-layer, layer + 1):\n",
    "            nearby_points.append((input_lat + i * step_size, input_lon + layer * step_size))\n",
    "        \n",
    "        # Right column (moving down)\n",
    "        for i in range(layer - 1, -layer - 1, -1):\n",
    "            nearby_points.append((input_lat + layer * step_size, input_lon + i * step_size))\n",
    "        \n",
    "        # Bottom row (moving left)\n",
    "        for i in range(layer - 1, -layer - 1, -1):\n",
    "            nearby_points.append((input_lat + i * step_size, input_lon - layer * step_size))\n",
    "        \n",
    "        # Left column (moving up)\n",
    "        for i in range(-layer + 1, layer):\n",
    "            nearby_points.append((input_lat - layer * step_size, input_lon + i * step_size))\n",
    "        \n",
    "        # Filter out points outside the map boundaries\n",
    "        nearby_points = [(lat, lon) for lat, lon in nearby_points if lon_min <= lon <= lon_max and lat_min <= lat <= lat_max]\n",
    "\n",
    "        # Check the nearby points for inundation status and margin safety\n",
    "        for nearby_lat, nearby_lon in nearby_points:\n",
    "            # Skip if the point is inundated\n",
    "            if check_inundation(inundation_map, nearby_lat, nearby_lon):\n",
    "                continue\n",
    "            \n",
    "            # Perform reverse geocoding to get the nearest place name\n",
    "            place_name = get_place_name(nearby_lat, nearby_lon)\n",
    "            if place_name not in unique_place_names and place_name != \"Unknown\":\n",
    "                # Add the safe location to the list\n",
    "                safe_locations.append((nearby_lat, nearby_lon, place_name))\n",
    "                unique_place_names.add(place_name)\n",
    "\n",
    "            # If we've found the required number of locations, return them\n",
    "            if len(safe_locations) >= max_locations:\n",
    "                return safe_locations\n",
    "        \n",
    "        # Move to the next outer layer\n",
    "        layer += 1\n",
    "    \n",
    "    # Return the found locations or None if no safe locations are found\n",
    "    return safe_locations if safe_locations else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e617ab16-a307-4cad-bf33-c48a23abff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario(scenario_name, file_path, inundation_map, input_lat, input_lon):\n",
    "    coordinates = (input_lat, input_lon)\n",
    "    bound_check = check_inundation(inundation_map[0], input_lat, input_lon, window_size=10)\n",
    "    output = \"\"\n",
    "    if bound_check == \"OUTSIDE BOUNDARY\":\n",
    "        output, ssh_change_df, highest_threshold_exceeded, next_output, inundation_occurred, inundation_year, coordinates, place_name, next_inundation_status = None, None, None, None, None, None, None, None, None\n",
    "        return output, ssh_change_df, highest_threshold_exceeded, next_output, inundation_occurred, inundation_year, coordinates, place_name, next_inundation_status\n",
    "        \n",
    "    slr_df = get_sea_level_projection(input_lat, input_lon, file_path)\n",
    "    \n",
    "    # Convert SSH_Change from meters to feet\n",
    "    mm_to_meters = 0.001\n",
    "    meters_to_feet = 3.28084\n",
    "    slr_df['Sea Level Rise'] = slr_df['Sea Level Rise'] * mm_to_meters * meters_to_feet\n",
    "\n",
    "    # Track thresholds exceeded, their corresponding years, and whether inundation occurs\n",
    "    threshold_exceedance = []\n",
    "\n",
    "    available_thresholds = sorted(inundation_map.keys())\n",
    "    highest_threshold_exceeded = 0\n",
    "    inundation_occurred = False\n",
    "    inundation_year = None\n",
    "    for threshold in available_thresholds:\n",
    "        exceedance_year = slr_df[slr_df['Sea Level Rise'] > threshold]['Year'].min()\n",
    "        if not pd.isna(exceedance_year):\n",
    "            file_path = inundation_map[threshold]\n",
    "            inundation_status = check_inundation(file_path, input_lat, input_lon, window_size=10)  # Pass window_size\n",
    "            threshold_exceedance.append((threshold, exceedance_year, inundation_status))\n",
    "            if inundation_status:\n",
    "                inundation_occurred = True\n",
    "                highest_threshold_exceeded = threshold\n",
    "                inundation_year = exceedance_year\n",
    "                break\n",
    "            highest_threshold_exceeded = threshold\n",
    "            \n",
    "    # Check the next threshold if applicable\n",
    "    if highest_threshold_exceeded < max(available_thresholds):\n",
    "        next_threshold_idx = available_thresholds.index(highest_threshold_exceeded) + 1\n",
    "        if next_threshold_idx < len(available_thresholds):\n",
    "            next_threshold = available_thresholds[next_threshold_idx]\n",
    "            next_file_path = inundation_map[next_threshold]\n",
    "            next_threshold_inundation = check_inundation(next_file_path, input_lat, input_lon, window_size=10)\n",
    "            next_inundation_status = \"Yes\" if next_threshold_inundation else \"No\"\n",
    "            next_output = f\"Next Unmet Threshold: {next_threshold} feet, Year Exceeded: Unknown, Inundation: {next_inundation_status}\"\n",
    "        else:\n",
    "            next_output = \"No higher threshold available to check.\"\n",
    "\n",
    "    place_name = get_place_name(input_lat, input_lon)\n",
    "    if threshold_exceedance:\n",
    "        for threshold, year, inundated in threshold_exceedance:\n",
    "            inundation_status = \"Yes\" if inundated else \"No\"\n",
    "            output += f\"Threshold: {threshold} feet, Year Exceeded: {year}, Inundation: {inundation_status}\"\n",
    "\n",
    "    return output, slr_df, highest_threshold_exceeded, next_output, inundation_occurred, inundation_year, coordinates, place_name, next_inundation_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99c93f06-d15e-4f0e-a03e-284d352b5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_safe_locations(input_lat, input_lon, inundation_map, inundation_occurance, highest_threshold_exceeded, find_radius=0.05, find_step=0.01, max_location=3, max_coord=5, whether_find_loc=True, whether_find_coord=False):\n",
    "    if inundation_occurance is None:\n",
    "        return None, None, None, None\n",
    "    nearby_safe_locations = None\n",
    "    nearby_safe_coordinates = None\n",
    "    min_safe_distance = None\n",
    "    safe_distance = None\n",
    "    safe_location = \"\"\n",
    "    safe_distances = []\n",
    "    if whether_find_loc:\n",
    "        nearby_safe_locations = find_closest_uninundated_locations(input_lat, input_lon, inundation_map[highest_threshold_exceeded+1], search_radius=find_radius, step_size=find_step, max_locations=max_location)\n",
    "        if nearby_safe_locations is not None:\n",
    "            for lat, lon, place in nearby_safe_locations:\n",
    "                if lat and lon:\n",
    "                    safe_distance = geodesic((input_lat, input_lon), (lat, lon)).miles\n",
    "                    safe_distances.append(safe_distance)\n",
    "                    safe_location += f\"{place} (coordinate: {lat}, {lon}). Distance: {safe_distance:.2f} miles.\"\n",
    "                else:\n",
    "                    safe_location += \"No safe location found within the search radius.\"\n",
    "            if safe_distances:\n",
    "                min_safe_distance = np.min(safe_distances)\n",
    "    return safe_location, nearby_safe_locations, nearby_safe_coordinates, min_safe_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "472ec079-b6d6-44e0-a423-f888b082d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'slr': 0.05, #Maximum SSH of the ocean near the location\n",
    "    'inundation': 0.5, #1, 0.5, 0, whether the inundation will be inundated\n",
    "    'proximity_safe_location': 0.1, #distance between the closest safe neighborhood and your house. Change it to only when knowing that the house is inundated, only when inundation score yields 0.5 or 0.\n",
    "    'storm': 0.20, #storm frequency\n",
    "    'flood': 0.15 #historical flood count\n",
    "}\n",
    "\n",
    "def calculate_risk_score(slr_score, inundation_score, proximity_score, storm_score, flood_score, weights):\n",
    "    # Combine the normalized risks using weights\n",
    "    if not slr_score:\n",
    "        total_score = \"Not enough data\"\n",
    "        return total_score\n",
    "    total_score = (\n",
    "        weights['slr'] * slr_score +\n",
    "        weights['inundation'] * inundation_score +\n",
    "        weights['proximity_safe_location'] * proximity_score +\n",
    "        weights['storm'] * storm_score +\n",
    "        weights['flood'] * flood_score\n",
    "    )\n",
    "    # Scale to 0-100\n",
    "    total_score *= 100\n",
    "    return total_score\n",
    "\n",
    "def calculate_slr_score(max_ssh_change, max_expected_slr=3.5):\n",
    "    if not max_ssh_change:\n",
    "        slr_score = None\n",
    "        return slr_score\n",
    "    # Normalize SLR (ensure it doesn't exceed max_expected_slr)\n",
    "    normalized_slr = min(max_ssh_change / max_expected_slr, 1)\n",
    "    # Invert to make higher values safer\n",
    "    slr_score = 1 - normalized_slr\n",
    "    return slr_score #Change it\n",
    "\n",
    "def calculate_inundation_score(inundation_year):\n",
    "    if inundation_year is None:\n",
    "        # Not inundated by 2100\n",
    "        return 1\n",
    "    elif inundation_year <= 2030:\n",
    "        # Inundated before or in 2050\n",
    "        return 0\n",
    "    elif inundation_year <= 2040:\n",
    "        return 0.125\n",
    "    elif inundation_year <= 2050:\n",
    "        return 0.25\n",
    "    elif inundation_year <= 2060:\n",
    "        return 0.375\n",
    "    elif inundation_year <= 2070:\n",
    "        return 0.5\n",
    "    elif inundation_year <= 2080:\n",
    "        return 0.625\n",
    "    elif inundation_year <= 2090:\n",
    "        return 0.75\n",
    "    elif inundation_year <= 2100:\n",
    "        return 0.875\n",
    "    else:\n",
    "        # Inundation year beyond 2100 (unlikely in your dataset)\n",
    "        return 1\n",
    "\n",
    "def calculate_proximity_score(min_safe_distance, inundation_year, max_distance=2):\n",
    "    if not min_safe_distance:\n",
    "        min_safe_distance = 2\n",
    "    if inundation_year:\n",
    "        # Normalize distance (ensure it doesn't exceed max_distance)\n",
    "        normalized_distance = min(min_safe_distance / max_distance, 1)\n",
    "        # Invert to make closer distances safer\n",
    "        proximity_score = 1 - normalized_distance\n",
    "        return proximity_score\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "def get_county_and_state(lat, lon):\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "        if location and location.address:\n",
    "            address = location.raw['address']\n",
    "            county_name = address.get('county', 'Unknown')\n",
    "            state_name = address.get('state', 'Unknown')\n",
    "            return county_name, state_name\n",
    "        else:\n",
    "            return \"Unknown\", \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "\n",
    "def normalize_county_name(county_name):\n",
    "    # Remove common suffixes\n",
    "    suffixes = ['County', 'Parish', 'Municipality', 'Borough', 'Census Area', 'City and Borough']\n",
    "    for suffix in suffixes:\n",
    "        if suffix in county_name:\n",
    "            county_name = county_name.replace(suffix, '')\n",
    "    # Strip whitespace and convert to uppercase\n",
    "    county_name = county_name.strip().upper()\n",
    "    return county_name\n",
    "\n",
    "usecols = [\n",
    "    'BEGIN_LAT', 'BEGIN_LON', 'EVENT_TYPE', 'BEGIN_DATE_TIME',\n",
    "    'EVENT_ID', 'CZ_NAME', 'CZ_TYPE', 'STATE'\n",
    "]\n",
    "storm_and_flood_df = pd.read_csv(storm_and_flood_csv, usecols=usecols)\n",
    "def get_storm_and_flood_frequency(lat, lon, df, radius_km=20):\n",
    "\n",
    "\n",
    "    # Filter for storm-related events\n",
    "    storm_types = ['Thunderstorm Wind', 'Tornado', 'Hail', 'Hurricane']\n",
    "    storm_events = df[df['EVENT_TYPE'].isin(storm_types)].copy()\n",
    "\n",
    "    # Filter for flood-related events\n",
    "    flood_types = ['Flood', 'Flash Flood', 'Coastal Flood']\n",
    "    flood_events = df[df['EVENT_TYPE'].isin(flood_types)].copy()\n",
    "\n",
    "\n",
    "    # Handle events with coordinates\n",
    "    storm_events_geo = storm_events.dropna(subset=['BEGIN_LAT', 'BEGIN_LON']).copy()\n",
    "    flood_events_geo = flood_events.dropna(subset=['BEGIN_LAT', 'BEGIN_LON']).copy()\n",
    "\n",
    "    # Create geometries\n",
    "    storm_geometry = [Point(xy) for xy in zip(storm_events_geo['BEGIN_LON'], storm_events_geo['BEGIN_LAT'])]\n",
    "    flood_geometry = [Point(xy) for xy in zip(flood_events_geo['BEGIN_LON'], flood_events_geo['BEGIN_LAT'])]\n",
    "\n",
    "    storm_gdf = gpd.GeoDataFrame(storm_events_geo, geometry=storm_geometry, crs='EPSG:4326')\n",
    "    flood_gdf = gpd.GeoDataFrame(flood_events_geo, geometry=flood_geometry, crs='EPSG:4326')\n",
    "\n",
    "    # Create a buffer around the input point\n",
    "    center_point = Point(lon, lat)\n",
    "    center_gdf = gpd.GeoDataFrame(index=[0], geometry=[center_point], crs='EPSG:4326')\n",
    "    buffer = center_gdf.to_crs(epsg=3857).buffer(radius_km * 1000)\n",
    "    buffer = buffer.to_crs(epsg=4326).unary_union\n",
    "   \n",
    "    # Find events within the buffer\n",
    "    storms_nearby = storm_gdf[storm_gdf.geometry.within(buffer)]\n",
    "    floods_nearby = flood_gdf[flood_gdf.geometry.within(buffer)]\n",
    "\n",
    "\n",
    "    county_storm_events = storm_events.iloc[0:0].copy()\n",
    "    county_flood_events = flood_events.iloc[0:0].copy()                     \n",
    "    county_name, state_name = get_county_and_state(lat, lon)\n",
    "    # Normalize state name\n",
    "    #state_abbr = get_state_abbreviation(state_name)\n",
    "    if county_name != \"Unknown\":\n",
    "        \n",
    "        county_name_norm = normalize_county_name(county_name)\n",
    "        # Normalize 'CZ_NAME' in the dataset\n",
    "        storm_events['CZ_NAME_NORM'] = storm_events['CZ_NAME'].str.upper().str.strip()\n",
    "        flood_events['CZ_NAME_NORM'] = flood_events['CZ_NAME'].str.upper().str.strip()\n",
    "\n",
    "        # Remove 'County' etc. from 'CZ_NAME_NORM'\n",
    "        storm_events['CZ_NAME_NORM'] = storm_events['CZ_NAME_NORM'].apply(normalize_county_name)\n",
    "        flood_events['CZ_NAME_NORM'] = flood_events['CZ_NAME_NORM'].apply(normalize_county_name)\n",
    "\n",
    "        # Get events affecting the county\n",
    "        county_storm_events = storm_events[\n",
    "            (storm_events['CZ_NAME_NORM'] == county_name_norm) &\n",
    "            (storm_events['CZ_TYPE'] == 'C') &\n",
    "            (storm_events['STATE'] == state_name.upper().strip())\n",
    "        ]\n",
    "\n",
    "        county_flood_events = flood_events[\n",
    "            (flood_events['CZ_NAME_NORM'] == county_name_norm) &\n",
    "            (flood_events['CZ_TYPE'] == 'C') &\n",
    "            (flood_events['STATE'] == state_name.upper().strip())\n",
    "        ]\n",
    "    \n",
    "    # Combine events and remove duplicates\n",
    "    storm_events_combined = pd.concat([storms_nearby, county_storm_events], ignore_index=True)\n",
    "    flood_events_combined = pd.concat([floods_nearby, county_flood_events], ignore_index=True)\n",
    "\n",
    "    # Remove duplicates based on 'EVENT_ID'\n",
    "    storm_events_combined = storm_events_combined.drop_duplicates(subset='EVENT_ID')\n",
    "    flood_events_combined = flood_events_combined.drop_duplicates(subset='EVENT_ID')\n",
    "    \n",
    "    # Return the counts\n",
    "    return len(storm_events_combined), len(flood_events_combined)\n",
    "\n",
    "def calculate_storm_score(storm_event_count, max_storm_event_count=45):\n",
    "    if not storm_event_count:\n",
    "        storm_event_count = 0\n",
    "    normalized_storm_event_count = min(storm_event_count / max_storm_event_count, 1)\n",
    "    storm_score = 1 - normalized_storm_event_count\n",
    "    return storm_score\n",
    "\n",
    "def calculate_flood_score(flood_event_count, max_flood_events=15):\n",
    "    if not flood_event_count:\n",
    "        flood_event_count = 0\n",
    "    normalized_flood_events = min(flood_event_count / max_flood_events, 1)\n",
    "    flood_score = 1 - normalized_flood_events\n",
    "    return flood_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e238dc4-2f21-4f10-aee0-3bcc68cbcd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0 feet, Year Exceeded: 2020, Inundation: NoThreshold: 0.5 feet, Year Exceeded: 2030, Inundation: NoThreshold: 1 feet, Year Exceeded: 2050, Inundation: NoThreshold: 1.5 feet, Year Exceeded: 2070, Inundation: NoThreshold: 2 feet, Year Exceeded: 2080, Inundation: NoThreshold: 2.5 feet, Year Exceeded: 2090, Inundation: NoThreshold: 3 feet, Year Exceeded: 2100, Inundation: No\n",
      "Next Unmet Threshold: 3.5 feet, Year Exceeded: Unknown, Inundation: No\n",
      "\n",
      "Maximum SLR is 3.13238199 feet at the year 2100\n",
      "\n",
      "Marion (coordinate: 41.676106999999995, -70.746257). Distance: 0.43 miles.Mattapoisett (coordinate: 41.671107, -70.761257). Distance: 0.86 miles.\n",
      "\n",
      "Nearby safe coordinates include (None)\n",
      "Threshold: 0 feet, Year Exceeded: 2020, Inundation: NoThreshold: 0.5 feet, Year Exceeded: 2030, Inundation: NoThreshold: 1 feet, Year Exceeded: 2050, Inundation: NoThreshold: 1.5 feet, Year Exceeded: 2070, Inundation: NoThreshold: 2 feet, Year Exceeded: 2090, Inundation: No\n",
      "Next Unmet Threshold: 2.5 feet, Year Exceeded: Unknown, Inundation: No\n",
      "\n",
      "Maximum SLR is 2.41797908 feet at the year 2100\n",
      "Marion (coordinate: 41.676106999999995, -70.746257). Distance: 0.43 miles.Mattapoisett (coordinate: 41.671107, -70.761257). Distance: 0.86 miles.\n",
      "\n",
      "Nearby safe coordinates include (None)\n",
      "-----------------\n",
      "\n",
      "the Risk Assessment Score (0 Most Dangerous, 100 Safest) for your locaton is: 89\n",
      "The function took 77.91 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "input_lat = 41.681107\n",
    "input_lon = -70.751257\n",
    "user_input_state = \"MA\"\n",
    "\n",
    "desired_zoom_factor = 0.0005\n",
    "whether_find_loc = True\n",
    "whether_get_score = True\n",
    "whether_find_coord = False\n",
    "find_radius = 0.02\n",
    "find_step = 0.005\n",
    "number_locations = 3\n",
    "number_coordinates = 0\n",
    "\n",
    "inundation_maps = state_inundation_maps.get(user_input_state)\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    if not inundation_maps:\n",
    "        no_result = print(f\"The state {user_input_state} either has no sea level rise and coastal flood data, or you don't need to worry about it.\")\n",
    "        return no_result  # Exit the function if no inundation maps are available\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "#______________________________________________________________________\n",
    "    # Process SSP585 Scenario\n",
    "    result_ssp585, ssh_change_df_ssp585, highest_threshold_ssp585, next_output_ssp585, inundation_occurred_ssp585, inundation_year_ssp585, coordinates_ssp585, place_name_ssp585, next_inundation_status_ssp585 = process_scenario(\"SSP585\", file_path_ar6_ssp585, inundation_maps, input_lat=input_lat, input_lon=input_lon)\n",
    "    if result_ssp585 is None:\n",
    "        bound_check = print(\"The data is unavailable or out of boundary\")\n",
    "        return bound_check  # Stop function execution here if no data is available\n",
    "    print(result_ssp585)\n",
    "    max_ssh_change_ssp585 = ssh_change_df_ssp585.loc[ssh_change_df_ssp585['Sea Level Rise'].idxmax()]\n",
    "    print(next_output_ssp585)\n",
    "    print(f\"\\nMaximum SLR is {max_ssh_change_ssp585['Sea Level Rise']} feet at the year {int(max_ssh_change_ssp585['Year'])}\\n\")\n",
    "    if whether_find_loc:\n",
    "        safe_location_ssp585, nearby_safe_locations_ssp585, nearby_safe_coordinates_ssp585, min_safe_distance_ssp585 = find_safe_locations(\n",
    "            input_lat, input_lon, inundation_maps, inundation_occurred_ssp585, highest_threshold_ssp585, \n",
    "            find_radius=find_radius, find_step=find_step, max_location=number_locations, \n",
    "            max_coord=number_coordinates, whether_find_loc=whether_find_loc, whether_find_coord=whether_find_coord\n",
    "        )\n",
    "        print(safe_location_ssp585)\n",
    "        print(f\"\\nNearby safe coordinates include ({nearby_safe_coordinates_ssp585})\")\n",
    "\n",
    "\n",
    "    # Process SSP245 Scenario\n",
    "    result_ssp245, ssh_change_df_ssp245, highest_threshold_ssp245, next_output_ssp245, inundation_occurred_ssp245, inundation_year_ssp245, coordinates_ssp585, place_name_ssp585, next_inundation_status_ssp585 = process_scenario(\"SSP245\", file_path_ar6_ssp245, inundation_maps, input_lat=input_lat, input_lon=input_lon)\n",
    "    if result_ssp245 is None:\n",
    "        bound_check = print(\"The data is unavailable or out of boundary\")\n",
    "        return bound_check  # Stop function execution here if no data is available\n",
    "    print(result_ssp245)\n",
    "    max_ssh_change_ssp245 = ssh_change_df_ssp245.loc[ssh_change_df_ssp245['Sea Level Rise'].idxmax()]\n",
    "    print(next_output_ssp245)\n",
    "    print(f\"\\nMaximum SLR is {max_ssh_change_ssp245['Sea Level Rise']} feet at the year {int(max_ssh_change_ssp245['Year'])}\")\n",
    "    if whether_find_loc or whether_find_coord:\n",
    "        safe_location_ssp245, nearby_safe_locations_ssp245, nearby_safe_coordinates_ssp245, min_safe_distance_ssp245 = find_safe_locations(\n",
    "            input_lat, input_lon, inundation_maps, inundation_occurred_ssp245, highest_threshold_ssp245, \n",
    "            find_radius=find_radius, find_step=find_step, max_location=number_locations, \n",
    "            max_coord=number_coordinates, whether_find_loc=whether_find_loc, whether_find_coord=whether_find_coord\n",
    "        )\n",
    "        print(safe_location_ssp245)\n",
    "        print(f\"\\nNearby safe coordinates include ({nearby_safe_coordinates_ssp245})\")\n",
    "\n",
    "#______________________________________________________________________\n",
    "    if whether_find_loc and whether_get_score:\n",
    "        slr_score = calculate_slr_score(max_ssh_change_ssp585['Sea Level Rise'])\n",
    "        inundation_score = calculate_inundation_score(inundation_year_ssp585)\n",
    "        proximity_score = calculate_proximity_score(min_safe_distance_ssp585, inundation_year_ssp585)\n",
    "        storm_count, flood_count = get_storm_and_flood_frequency(input_lat, input_lon, storm_and_flood_df)\n",
    "        storm_score = calculate_storm_score(storm_count)\n",
    "        flood_score = calculate_flood_score(flood_count)\n",
    "        risk_assessment_score = calculate_risk_score(slr_score, inundation_score, proximity_score, storm_score, flood_score, weights)\n",
    "        print(f\"-----------------\\n\\nthe Risk Assessment Score (0 Most Dangerous, 100 Safest) for your locaton is: {round(risk_assessment_score)}\")  \n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"The function took {elapsed_time:.2f} seconds to run.\")\n",
    "\n",
    "run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "502aa7dc-aab4-4dd3-ade6-0b9f546599ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'slr': 0.1, #Maximum SSH of the ocean near the location\n",
    "    'inundation': 0.5, #1, 0.5, 0, whether the inundation will be inundated\n",
    "    'proximity_safe_location': 0.1, #distance between the closest safe neighborhood and your house. Change it to only when knowing that the house is inundated, only when inundation score yields 0.5 or 0.\n",
    "    'storm': 0.15, #storm frequency\n",
    "    'flood': 0.15 #historical flood count\n",
    "}\n",
    "\n",
    "blank_points = [\n",
    "    (), (), (), (),\n",
    "    (), (), (), (),\n",
    "    (), (), (), (),\n",
    "    (), (), (), (),\n",
    "    (), (), (), (),\n",
    "    (), (), (), (),\n",
    "    (), (), (), ()\n",
    "]\n",
    "\n",
    "def find_many_points(points, scenario = \"SSP585\", file_path = file_path_ar6_ssp585, inundation_maps = None):\n",
    "    slr_list = []\n",
    "    inundation_list = []\n",
    "    proximity_list = []\n",
    "    storm_list = []\n",
    "    flood_list = []\n",
    "    risk_assessment_score = []\n",
    "    for input_lat, input_lon in points:\n",
    "        output, ssh_change_df, highest_threshold, next_output, inundation_occurred, inundation_year, coordinates, place_name, next_inundation_status = process_scenario(scenario, file_path, inundation_maps, input_lat=input_lat, input_lon=input_lon)\n",
    "        inundation_list.append(inundation_occurred)\n",
    "        if ssh_change_df is not None:\n",
    "            max_ssh_change = ssh_change_df.loc[ssh_change_df['Sea Level Rise'].idxmax()]\n",
    "            max_ssh_change_value = max_ssh_change['Sea Level Rise']\n",
    "            slr_list.append(max_ssh_change['Sea Level Rise'])\n",
    "        else:\n",
    "            max_ssh_change = None\n",
    "            max_ssh_change_value = None\n",
    "            slr_list.append(max_ssh_change)\n",
    "            \n",
    "        safe_location, nearby_safe_locations, nearby_safe_coordinates, min_safe_distance = find_safe_locations(input_lat, input_lon, inundation_maps, \n",
    "                                                                                                               inundation_occurred, highest_threshold, \n",
    "                                                                                                               find_radius=0.02, find_step=0.005, max_location=3, \n",
    "                                                                                                               max_coord=0, whether_find_loc=True, whether_find_coord=False\n",
    "                                                                                                              )\n",
    "        proximity_list.append(min_safe_distance)\n",
    "        storm_count, flood_count = get_storm_and_flood_frequency(input_lat, input_lon, storm_and_flood_df, radius_km=20)\n",
    "        storm_list.append(storm_count)\n",
    "        flood_list.append(flood_count)\n",
    "\n",
    "        slr_score = calculate_slr_score(max_ssh_change_value, max_expected_slr=3.5)\n",
    "        inundation_score = calculate_inundation_score(inundation_year)\n",
    "        proximity_score = calculate_proximity_score(min_safe_distance, inundation_year, max_distance=2)\n",
    "        storm_score = calculate_storm_score(storm_count, max_storm_event_count=45)\n",
    "        flood_score = calculate_flood_score(flood_count, max_flood_events=15)\n",
    "        risk_score = calculate_risk_score(slr_score, inundation_score, proximity_score, storm_score, flood_score, weights)\n",
    "        risk_assessment_score.append(risk_score)\n",
    "        \n",
    "    return slr_list, inundation_list, proximity_list, storm_list, flood_list, risk_assessment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a863d94-6ae4-4603-aa6f-c06eec0cb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_year_score(scenario, file_path, inundation_maps, input_lat, input_lon):\n",
    "    slr_df = get_sea_level_projection(input_lat, input_lon, file_path)\n",
    "    mm_to_meters = 0.001\n",
    "    meters_to_feet = 3.28084\n",
    "    slr_df['Sea Level Rise'] = slr_df['Sea Level Rise'] * mm_to_meters * meters_to_feet\n",
    "    \n",
    "    slr_year_df = slr_df['Year'].tolist()\n",
    "    slr_value_df = slr_df['Sea Level Rise'].tolist()\n",
    "    slr_list = list(zip(slr_year_df, slr_value_df))\n",
    "\n",
    "    available_thresholds = sorted(inundation_maps.keys())\n",
    "    highest_threshold_minus = max(available_thresholds) - 1\n",
    "    \n",
    "    storm_count, flood_count = get_storm_and_flood_frequency(input_lat, input_lon, storm_and_flood_df, radius_km=20)\n",
    "    storm_score = calculate_storm_score(storm_count, max_storm_event_count=45)\n",
    "    flood_score = calculate_flood_score(flood_count, max_flood_events=15)\n",
    "\n",
    "    safe_location, nearby_safe_locations, nearby_safe_coordinates, min_safe_distance = find_safe_locations(input_lat, input_lon, inundation_maps, \n",
    "                                                                                                               inundation_occurance=True, highest_threshold_exceeded=highest_threshold_minus, \n",
    "                                                                                                               find_radius=0.02, find_step=0.005, max_location=3, \n",
    "                                                                                                               max_coord=0, whether_find_loc=True, whether_find_coord=False\n",
    "                                                                                                              )\n",
    "    proximity_score = calculate_proximity_score(min_safe_distance, inundation_year=True, max_distance=2)\n",
    "    slr_risk_scores = []\n",
    "    inundation_risk_scores = []\n",
    "    proximity_risk_scores = []\n",
    "    flood_risk_scores = []\n",
    "    storm_risk_scores = []\n",
    "    year_risk_scores = []\n",
    "    \n",
    "    inundation_year_list = []\n",
    "    for year,slr in slr_list:\n",
    "        inundation_year = None\n",
    "        slr_score = calculate_slr_score(slr, max_expected_slr=3.5)\n",
    "        inundation_occurred = False\n",
    "        for threshold in available_thresholds:\n",
    "            if slr >= threshold:\n",
    "                inundation_occurred = check_inundation(inundation_maps[threshold], input_lat, input_lon, window_size=10)\n",
    "                if inundation_occurred:\n",
    "                    break\n",
    "                    \n",
    "        if inundation_occurred:\n",
    "            inundation_year = year\n",
    "            inundation_year_list.append(inundation_year)\n",
    "        if not inundation_occurred:\n",
    "                proximity_score = 1\n",
    "            \n",
    "        # inundation_score = calculate_inundation_score(inundation_year)\n",
    "        # if inundation_year:\n",
    "        #     if any(yr < year for yr in inundation_year_list):\n",
    "        #         if any(yr <= 2030 for yr in inundation_year_list):\n",
    "        #             inundation_score = 0\n",
    "        #         inundation_score = calculate_inundation_score(yr)\n",
    "\n",
    "        first_inundation_year = min(inundation_year_list) if inundation_year_list else None\n",
    "\n",
    "        inundation_score = calculate_inundation_score(first_inundation_year) if first_inundation_year else 1\n",
    "                \n",
    "        risk_score = calculate_risk_score(slr_score, inundation_score, proximity_score, storm_score, flood_score, weights)\n",
    "        risk_score = round(risk_score)\n",
    "        year_risk_scores.append(risk_score)\n",
    "        storm_risk_scores.append(round(storm_score*100))\n",
    "        flood_risk_scores.append(round(flood_score*100))\n",
    "        slr_risk_scores.append(round(slr_score*100))\n",
    "        inundation_risk_scores.append(round(inundation_score*100))\n",
    "        proximity_risk_scores.append(round(proximity_score*100))\n",
    "    return year_risk_scores, slr_risk_scores, inundation_risk_scores, proximity_risk_scores, storm_risk_scores, flood_risk_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3fe9d0d4-dbf9-4e5e-ab5f-82019ee4974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_year_scores_for_all_points(points, scenario, file_path, inundation_maps):\n",
    "    # List to store the year scores for each point\n",
    "    all_year_scores = []\n",
    "    all_slr_scores = []\n",
    "    all_inundation_scores = []\n",
    "    all_proximity_scores = []\n",
    "    all_storm_scores = []\n",
    "    all_flood_scores = []\n",
    "    # Loop through each point (lat, lon) in the points list\n",
    "    for idx, (input_lat, input_lon) in enumerate(points, start=1):\n",
    "        # Get year scores for the current point\n",
    "        year_scores, slr_scores, inundation_scores, proximity_scores, storm_scores, flood_scores = calculate_year_score(scenario, file_path, inundation_maps, input_lat, input_lon)\n",
    "        \n",
    "        # Append the year scores to the list\n",
    "        all_year_scores.append(year_scores)\n",
    "        all_slr_scores.append(slr_scores)\n",
    "        all_inundation_scores.append(inundation_scores)\n",
    "        all_proximity_scores.append(proximity_scores)\n",
    "        all_storm_scores.append(storm_scores)\n",
    "        all_flood_scores.append(flood_scores)\n",
    "    # Define the years corresponding to the scores (2020 to 2100)\n",
    "    years = [2020, 2030, 2040, 2050, 2060, 2070, 2080, 2090, 2100]\n",
    "    \n",
    "    # Create a DataFrame with each row representing the scores for a point and columns as years\n",
    "    df_year_scores = pd.DataFrame(all_year_scores, columns=years)\n",
    "    df_slr_scores = pd.DataFrame(all_slr_scores, columns=years)\n",
    "    df_inundation_scores = pd.DataFrame(all_inundation_scores, columns=years)\n",
    "    df_proximity_scores = pd.DataFrame(all_proximity_scores, columns=years)\n",
    "    df_storm_scores = pd.DataFrame(all_storm_scores, columns=years)\n",
    "    df_flood_scores = pd.DataFrame(all_flood_scores, columns=years)\n",
    "    return df_year_scores, df_slr_scores, df_inundation_scores, df_proximity_scores, df_storm_scores, df_flood_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3c54133-e937-489b-8068-c70ef4a89933",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_points = [(41.560607, -70.936777), (41.565849, -70.939113), (41.581573, -70.943054), (41.596859, -70.928495), \n",
    "            (41.606234, -70.904464), (41.608318, -70.864587), (41.653431, -70.792897), (41.681019, -70.758942),\n",
    "             (41.715625, -70.614196), (41.748468, -70.436862), (42.010510, -70.082880), (41.924087, -70.544832),\n",
    "             (42.249851, -70.979433), (42.356573, -71.017367), (42.468619, -70.900101), (42.937922, -70.801979),\n",
    "             (42.557918, -70.817436), (41.588883, -70.642907), (41.574060, -70.464419), (41.844237, -70.001492),\n",
    "             (42.106916, -70.664196), (42.315835, -71.033523), (42.341551, -71.018082), (42.368306, -71.055174),\n",
    "            (42.382859, -70.973359), (42.415886, -70.989154), (42.276716, -71.011516), (42.299167, -71.026601)\n",
    "            ]\n",
    "points = MA_points\n",
    "inundation_maps = inundation_maps_MA\n",
    "df_year_scores_MA_585, df_slr_scores_MA_585, df_inundation_scores_MA_585, df_proximity_scores_MA_585, df_storm_scores_MA_585, df_flood_scores_MA_585 = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_MA_245, df_slr_scores_MA_245, df_inundation_scores_MA_245, df_proximity_scores_MA_245, df_storm_scores_MA_245, df_flood_scores_MA_245 = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "FL_points = [\n",
    "    (25.710288, -80.248741), (25.711359, -80.249031), (25.713607, -80.249205), (25.716430, -80.246159),\n",
    "    (25.722651, -80.241691), (25.727879, -80.233596), (25.731930, -80.232030), (25.734883, -80.226169),\n",
    "    (25.739692, -80.217610), (25.735458, -80.219815), (25.750197, -80.200058), (25.756207, -80.192631),\n",
    "    (25.764020, -80.188511), (25.769376, -80.183724), (25.773021, -80.174953), (25.769636, -80.166171),\n",
    "    (25.789411, -80.185508), (25.787964, -80.185874), (25.767063, -80.145641), (25.801229, -80.185612),\n",
    "    (25.798117, -80.185488), (25.750499, -80.146885), (25.808858, -80.185458), (25.814029, -80.185438),\n",
    "    (25.828122, -80.180148), (25.829501, -80.180148), (25.831493, -80.180537), (25.843260, -80.174125)\n",
    "]\n",
    "points = FL_points\n",
    "inundation_maps = inundation_maps_FL\n",
    "df_year_scores_FL_585, df_slr_scores_FL_585, df_inundation_scores_FL_585, df_proximity_scores_FL_585, df_storm_scores_FL_585, df_flood_scores_FL_585 = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_FL_245, df_slr_scores_FL_245, df_inundation_scores_FL_245, df_proximity_scores_FL_245, df_storm_scores_FL_245, df_flood_scores_FL_245 = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "CA_points = [\n",
    "    (33.995670, -118.481175), (33.994332, -118.479910), (33.992260, -118.478106), (33.984547, -118.471431),\n",
    "    (33.983441, -118.470589), (33.980933, -118.468220), (33.978273, -118.465994), (33.966969, -118.453742),\n",
    "    (34.027915, -118.518500), (34.027003, -118.516969), (34.038811, -118.554159), (34.039654, -118.558538),\n",
    "    (34.041683, -118.564441), (34.042483, -118.568966), (34.041887, -118.576805), (34.039840, -118.549802),\n",
    "    (34.000963, -118.484727), (34.025433, -118.512226), (34.024745, -118.510618), (34.022617, -118.507740),\n",
    "    (34.016728, -118.500791), (33.997855, -118.482665), (33.713437, -118.310474), (33.712465, -118.307591),\n",
    "    (33.707400, -118.286125), (33.717434, -118.257453), (33.727250, -118.267017), (33.740619, -118.252977)\n",
    "]\n",
    "points = CA_points\n",
    "inundation_maps = inundation_maps_CA\n",
    "df_year_scores_CA_585, df_slr_scores_CA_585, df_inundation_scores_CA_585, df_proximity_scores_CA_585, df_storm_scores_CA_585, df_flood_scores_CA_585 = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_CA_245, df_slr_scores_CA_245, df_inundation_scores_CA_245, df_proximity_scores_CA_245, df_storm_scores_CA_245, df_flood_scores_CA_245 = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "NY_points = [\n",
    "    (40.895381, -73.914268), (40.889338, -73.916972), (40.872985, -73.826694), (40.853694, -73.937895),\n",
    "    (40.870052, -73.893814), (40.820718, -73.907498), (40.785664, -73.948408), (40.761136, -73.985058),\n",
    "    (40.735954, -73.986478), (40.720238, -73.980796), (40.779210, -73.816869), (40.885186, -73.614587),\n",
    "    (40.584352, -73.935881), (40.599723, -74.003273), (40.606774, -74.028992), (40.634430, -74.036374),\n",
    "    (40.678190, -74.017009), (40.730229, -74.009150), (40.710025, -73.983097), (40.703136, -74.011422),\n",
    "    (40.739407, -74.011275), (40.765401, -73.997449), (40.774759, -73.944583), (40.803061, -73.970547),\n",
    "    (40.701001, -73.972433), (40.574303, -74.006938), (40.805118, -73.885627), (40.808528, -73.857165),\n",
    "    (40.799545, -73.911341), (40.795670, -73.800737), (40.777185, -73.768336), (40.862436, -73.694524),\n",
    "    (40.612986, -73.766800), (40.617175, -73.755617), (40.627645, -73.748375), (40.641665, -73.722265),\n",
    "    (40.877230, -73.653813), (40.824890, -73.701513), (40.628445, -73.595870), (40.592314, -73.674647)   \n",
    "]\n",
    "points = NY_points\n",
    "inundation_maps = inundation_maps_NY\n",
    "df_year_scores_NY_585, df_slr_scores_NY_585, df_inundation_scores_NY_585, df_proximity_scores_NY_585, df_storm_scores_NY_585, df_flood_scores_NY_585 = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_NY_245, df_slr_scores_NY_245, df_inundation_scores_NY_245, df_proximity_scores_NY_245, df_storm_scores_NY_245, df_flood_scores_NY_245 = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "PA_points = [(39.815000, -75.413719),(39.828980, -75.384748),(39.978452, -75.097862),(39.938166, -75.143754),\n",
    "            (39.920865, -75.138882),(39.890382, -75.146830),(39.966467, -75.132473),(40.003790, -75.059918),\n",
    "            (40.047085, -74.983793),(40.061286, -74.960407),(40.066417, -74.945752),(40.077320, -74.897452),\n",
    "            (40.086982, -74.862997),(40.105944, -74.838364),(40.125856, -74.829165),(40.126928, -74.778964),\n",
    "            (40.200917, -74.766179),(40.228537, -74.798452),(40.359194, -74.949356),(39.945794, -75.141224),\n",
    "            (39.937123, -75.142637),(39.953180, -75.140805),(39.967396, -75.125977),(39.960803, -75.137608)]\n",
    "points = PA_points\n",
    "inundation_maps = inundation_maps_PA\n",
    "df_year_scores_PA_585, df_slr_scores_PA_585, df_inundation_scores_PA_585, df_proximity_scores_PA_585, df_storm_scores_PA_585, df_flood_scores_PA_585 = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_PA_245, df_slr_scores_PA_245, df_inundation_scores_PA_245, df_proximity_scores_PA_245, df_storm_scores_PA_245, df_flood_scores_PA_245 = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "BOS_points = [\n",
    "    (42.331558, -71.078278), (42.340241, -71.052201), (42.378432, -71.090024), (42.378825, -71.105728),\n",
    "    (42.357453, -71.105043), (42.358496, -71.101579), (42.364750, -71.093306), (42.358401, -71.064636),\n",
    "    (42.382378, -71.062199), (42.372239, -70.988825), (42.377925, -71.051167), (42.358780, -71.114792),\n",
    "    (42.388091, -71.074312), (42.349515, -71.113738), (42.354634, -71.102550), (42.353059, -71.047851),\n",
    "    (42.322471, -71.047140), (42.354240, -71.075556), (42.385992, -71.072359), (42.390452, -71.066853),\n",
    "    (42.371430, -71.110364), (42.415521, -71.049916), (42.348700, -71.116093), (42.366163, -71.077556),\n",
    "    (42.385280, -71.005802), (42.399262, -71.087397), (42.341084, -71.025091), (42.359990, -70.973449)\n",
    "]\n",
    "points = BOS_points\n",
    "inundation_maps = inundation_maps_MA\n",
    "df_year_scores_BOS_585, df_slr_scores_BOS_585, df_inundation_scores_BOS_585, df_proximity_scores_BOS_585, df_storm_scores_BOS_585, df_flood_scores_BOS_585 = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_BOS_245, df_slr_scores_BOS_245, df_inundation_scores_BOS_245, df_proximity_scores_BOS_245, df_storm_scores_BOS_245, df_flood_scores_BOS_245 = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "939230cf-bee2-48bc-85e3-62c59e49e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_points_new = [\n",
    "    (39.936491, -75.151825), (39.923660, -75.142007), (39.963039, -75.136260), (39.971029, -75.118018),\n",
    "    (39.961791, -75.144567), (39.932538, -75.161290), (39.980130, -75.143522), (39.939115, -75.152917),\n",
    "    (39.921575, -75.179059), (39.967989, -75.215491), (40.006568, -75.069793), (39.997130, -75.068721),\n",
    "    (39.913367, -75.137821), (40.022567, -75.047831), (40.073413, -74.980874), (40.004106, -75.064436)\n",
    "]\n",
    "points = PA_points_new\n",
    "inundation_maps = inundation_maps_PA\n",
    "df_year_scores_PA_585_new, df_slr_scores_PA_585_new, df_inundation_scores_PA_585_new, df_proximity_scores_PA_585_new, df_storm_scores_PA_585_new, df_flood_scores_PA_585_new = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_PA_245_new, df_slr_scores_PA_245_new, df_inundation_scores_PA_245_new, df_proximity_scores_PA_245_new, df_storm_scores_PA_245_new, df_flood_scores_PA_245_new = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "MA_points_new = [\n",
    "    (42.344192, -71.075606), (42.340543, -71.051049), (42.368231, -71.081049), (42.351021, -71.085352),\n",
    "    (42.380976, -71.067039), (42.380976, -71.037028), (42.323180, -71.074242), (42.284065, -71.130862),\n",
    "    (42.362730, -71.125262), (42.350314, -71.10141), (42.352079, -71.083222), (42.377704, -71.127219)\n",
    "]\n",
    "points = MA_points_new\n",
    "inundation_maps = inundation_maps_MA\n",
    "df_year_scores_MA_585_new, df_slr_scores_MA_585_new, df_inundation_scores_MA_585_new, df_proximity_scores_MA_585_new, df_storm_scores_MA_585_new, df_flood_scores_MA_585_new = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_MA_245_new, df_slr_scores_MA_245_new, df_inundation_scores_MA_245_new, df_proximity_scores_MA_245_new, df_storm_scores_MA_245_new, df_flood_scores_MA_245_new = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "BOS_points_new = [\n",
    "    (42.344192, -71.075606), (42.340543, -71.051049), (42.368231, -71.081049), (42.351021, -71.085352),\n",
    "    (42.380976, -71.067039), (42.380976, -71.037028), (42.323180, -71.074242), (42.284065, -71.130862),\n",
    "    (42.362730, -71.125262), (42.350314, -71.10141), (42.352079, -71.083222), (42.377704, -71.127219)\n",
    "]\n",
    "df_year_scores_BOS_585_new, df_slr_scores_BOS_585_new, df_inundation_scores_BOS_585_new, df_proximity_scores_BOS_585_new, df_storm_scores_BOS_585_new, df_flood_scores_BOS_585_new = df_year_scores_MA_585_new.copy(), df_slr_scores_MA_585_new.copy(), df_inundation_scores_MA_585_new.copy(), df_proximity_scores_MA_585_new.copy(), df_storm_scores_MA_585_new.copy(), df_flood_scores_MA_585_new.copy()\n",
    "df_year_scores_BOS_245_new, df_slr_scores_BOS_245_new, df_inundation_scores_BOS_245_new, df_proximity_scores_BOS_245_new, df_storm_scores_BOS_245_new, df_flood_scores_BOS_245_new = df_year_scores_MA_245_new.copy(), df_slr_scores_MA_245_new.copy(), df_inundation_scores_MA_245_new.copy(), df_proximity_scores_MA_245_new.copy(), df_storm_scores_MA_245_new.copy(), df_flood_scores_MA_245_new.copy()\n",
    "\n",
    "\n",
    "FL_points_new = [\n",
    "    (25.827304, -80.181043), (25.830703, -80.180247), (25.832638, -80.181097), (25.842615, -80.176481),\n",
    "    (25.854386, -80.176362), (25.830564, -80.186898), (25.817687, -80.191609), (25.816608, -80.185784),\n",
    "    (25.828559, -80.182444), (25.827689, -80.180030), (25.832035, -80.180866), (25.820023, -80.179497)\n",
    "]\n",
    "points = FL_points_new\n",
    "inundation_maps = inundation_maps_FL\n",
    "df_year_scores_FL_585_new, df_slr_scores_FL_585_new, df_inundation_scores_FL_585_new, df_proximity_scores_FL_585_new, df_storm_scores_FL_585_new, df_flood_scores_FL_585_new = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_FL_245_new, df_slr_scores_FL_245_new, df_inundation_scores_FL_245_new, df_proximity_scores_FL_245_new, df_storm_scores_FL_245_new, df_flood_scores_FL_245_new = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)\n",
    "\n",
    "\n",
    "CA_points_new = [\n",
    "    (33.733449, -118.256768), (33.779885, -118.272089), (34.067814, -118.337867), (34.011927, -118.300462),\n",
    "    (34.204323, -118.577602), (34.032126, -118.436482), (34.008169, -118.444983), (33.960956, -118.434747),\n",
    "    (33.955669, -118.443033), (33.962542, -118.430923), (34.061341, -118.437297), (34.032823, -118.518881)\n",
    "]\n",
    "points = CA_points_new\n",
    "inundation_maps = inundation_maps_CA\n",
    "df_year_scores_CA_585_new, df_slr_scores_CA_585_new, df_inundation_scores_CA_585_new, df_proximity_scores_CA_585_new, df_storm_scores_CA_585_new, df_flood_scores_CA_585_new = calculate_year_scores_for_all_points(points, 'SSP585', file_path_ar6_ssp585, inundation_maps)\n",
    "df_year_scores_CA_245_new, df_slr_scores_CA_245_new, df_inundation_scores_CA_245_new, df_proximity_scores_CA_245_new, df_storm_scores_CA_245_new, df_flood_scores_CA_245_new = calculate_year_scores_for_all_points(points, 'SSP245', file_path_ar6_ssp245, inundation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9bf95af6-0bc0-438e-8e86-124951ea89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_year_scores_MA_585['State'] = 'MA'\n",
    "# df_year_scores_MA_585['Scenario'] = 'SSP585'\n",
    "# df_year_scores_MA_585_new['State'] = 'MA'\n",
    "# df_year_scores_MA_585_new['Scenario'] = 'SSP585'\n",
    "\n",
    "# df_year_scores_MA_245['State'] = 'MA'\n",
    "# df_year_scores_MA_245['Scenario'] = 'SSP245'\n",
    "# df_year_scores_MA_245_new['State'] = 'MA'\n",
    "# df_year_scores_MA_245_new['Scenario'] = 'SSP245'\n",
    "\n",
    "# df_year_scores_PA_585['State'] = 'PA'\n",
    "# df_year_scores_PA_585['Scenario'] = 'SSP585'\n",
    "# df_year_scores_PA_585_new['State'] = 'PA'\n",
    "# df_year_scores_PA_585_new['Scenario'] = 'SSP585'\n",
    "\n",
    "# df_year_scores_PA_245['State'] = 'PA'\n",
    "# df_year_scores_PA_245['Scenario'] = 'SSP245'\n",
    "# df_year_scores_PA_245_new['State'] = 'PA'\n",
    "# df_year_scores_PA_245_new['Scenario'] = 'SSP245'\n",
    "\n",
    "# df_year_scores_FL_585['State'] = 'FL'\n",
    "# df_year_scores_FL_585['Scenario'] = 'SSP585'\n",
    "# df_year_scores_FL_585_new['State'] = 'FL'\n",
    "# df_year_scores_FL_585_new['Scenario'] = 'SSP585'\n",
    "\n",
    "# df_year_scores_FL_245['State'] = 'FL'\n",
    "# df_year_scores_FL_245['Scenario'] = 'SSP245'\n",
    "# df_year_scores_FL_245_new['State'] = 'FL'\n",
    "# df_year_scores_FL_245_new['Scenario'] = 'SSP245'\n",
    "\n",
    "# df_year_scores_CA_585['State'] = 'CA'\n",
    "# df_year_scores_CA_585['Scenario'] = 'SSP585'\n",
    "# df_year_scores_CA_585_new['State'] = 'CA'\n",
    "# df_year_scores_CA_585_new['Scenario'] = 'SSP585'\n",
    "\n",
    "# df_year_scores_CA_245['State'] = 'CA'\n",
    "# df_year_scores_CA_245['Scenario'] = 'SSP245'\n",
    "# df_year_scores_CA_245_new['State'] = 'CA'\n",
    "# df_year_scores_CA_245_new['Scenario'] = 'SSP245'\n",
    "\n",
    "# df_year_scores_NY_585['State'] = 'NY'\n",
    "# df_year_scores_NY_585['Scenario'] = 'SSP585'\n",
    "\n",
    "# df_year_scores_NY_245['State'] = 'NY'\n",
    "# df_year_scores_NY_245['Scenario'] = 'SSP245'\n",
    "\n",
    "# df_year_scores_BOS_585['State'] = 'BOS'\n",
    "# df_year_scores_BOS_585['Scenario'] = 'SSP585'\n",
    "# df_year_scores_BOS_585_new['State'] = 'BOS'\n",
    "# df_year_scores_BOS_585_new['Scenario'] = 'SSP585'\n",
    "\n",
    "# df_year_scores_BOS_245['State'] = 'BOS'\n",
    "# df_year_scores_BOS_245['Scenario'] = 'SSP245'\n",
    "# df_year_scores_BOS_245_new['State'] = 'BOS'\n",
    "# df_year_scores_BOS_245_new['Scenario'] = 'SSP245'\n",
    "\n",
    "\n",
    "# # Concatenate all DataFrames together\n",
    "# combined_df = pd.concat([\n",
    "#     df_year_scores_MA_585, df_year_scores_MA_585_new, df_year_scores_MA_245, df_year_scores_MA_245_new,\n",
    "#     df_year_scores_PA_585, df_year_scores_PA_585_new, df_year_scores_PA_245, df_year_scores_PA_245_new,\n",
    "#     df_year_scores_FL_585, df_year_scores_FL_585_new, df_year_scores_FL_245, df_year_scores_FL_245_new,\n",
    "#     df_year_scores_CA_585, df_year_scores_CA_585_new, df_year_scores_CA_245, df_year_scores_CA_245_new,\n",
    "#     df_year_scores_NY_585, df_year_scores_NY_245,\n",
    "#     df_year_scores_BOS_585, df_year_scores_BOS_585_new, df_year_scores_BOS_245, df_year_scores_BOS_245_new,\n",
    "# ], ignore_index=True)\n",
    "\n",
    "# # Export the combined DataFrame to an Excel file\n",
    "# combined_df.to_excel('combined_year_scores_new.xlsx', index=False)\n",
    "# combined_df.to_csv('combined_year_scores_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95438f64-6904-45c7-8edf-6ac95cb6f82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to 'combined_scores_new.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add 'State' and 'Scenario' columns for each score type\n",
    "# Year Scores\n",
    "for df, state, scenario in [\n",
    "    (df_year_scores_MA_585, 'MA', 'SSP585'), (df_year_scores_MA_585_new, 'MA', 'SSP585'),\n",
    "    (df_year_scores_MA_245, 'MA', 'SSP245'), (df_year_scores_MA_245_new, 'MA', 'SSP245'),\n",
    "    (df_year_scores_PA_585, 'PA', 'SSP585'), (df_year_scores_PA_585_new, 'PA', 'SSP585'),\n",
    "    (df_year_scores_PA_245, 'PA', 'SSP245'), (df_year_scores_PA_245_new, 'PA', 'SSP245'),\n",
    "    (df_year_scores_FL_585, 'FL', 'SSP585'), (df_year_scores_FL_585_new, 'FL', 'SSP585'),\n",
    "    (df_year_scores_FL_245, 'FL', 'SSP245'), (df_year_scores_FL_245_new, 'FL', 'SSP245'),\n",
    "    (df_year_scores_CA_585, 'CA', 'SSP585'), (df_year_scores_CA_585_new, 'CA', 'SSP585'),\n",
    "    (df_year_scores_CA_245, 'CA', 'SSP245'), (df_year_scores_CA_245_new, 'CA', 'SSP245'),\n",
    "    (df_year_scores_NY_585, 'NY', 'SSP585'), (df_year_scores_NY_245, 'NY', 'SSP245'),\n",
    "    (df_year_scores_BOS_585, 'BOS', 'SSP585'), (df_year_scores_BOS_585_new, 'BOS', 'SSP585'),\n",
    "    (df_year_scores_BOS_245, 'BOS', 'SSP245'), (df_year_scores_BOS_245_new, 'BOS', 'SSP245')\n",
    "]:\n",
    "    df['State'] = state\n",
    "    df['Scenario'] = scenario\n",
    "\n",
    "# SLR Scores\n",
    "for df, state, scenario in [\n",
    "    (df_slr_scores_MA_585, 'MA', 'SSP585'), (df_slr_scores_MA_585_new, 'MA', 'SSP585'),\n",
    "    (df_slr_scores_MA_245, 'MA', 'SSP245'), (df_slr_scores_MA_245_new, 'MA', 'SSP245'),\n",
    "    (df_slr_scores_PA_585, 'PA', 'SSP585'), (df_slr_scores_PA_585_new, 'PA', 'SSP585'),\n",
    "    (df_slr_scores_PA_245, 'PA', 'SSP245'), (df_slr_scores_PA_245_new, 'PA', 'SSP245'),\n",
    "    (df_slr_scores_FL_585, 'FL', 'SSP585'), (df_slr_scores_FL_585_new, 'FL', 'SSP585'),\n",
    "    (df_slr_scores_FL_245, 'FL', 'SSP245'), (df_slr_scores_FL_245_new, 'FL', 'SSP245'),\n",
    "    (df_slr_scores_CA_585, 'CA', 'SSP585'), (df_slr_scores_CA_585_new, 'CA', 'SSP585'),\n",
    "    (df_slr_scores_CA_245, 'CA', 'SSP245'), (df_slr_scores_CA_245_new, 'CA', 'SSP245'),\n",
    "    (df_slr_scores_NY_585, 'NY', 'SSP585'), (df_slr_scores_NY_245, 'NY', 'SSP245'),\n",
    "    (df_slr_scores_BOS_585, 'BOS', 'SSP585'), (df_slr_scores_BOS_585_new, 'BOS', 'SSP585'),\n",
    "    (df_slr_scores_BOS_245, 'BOS', 'SSP245'), (df_slr_scores_BOS_245_new, 'BOS', 'SSP245')\n",
    "]:\n",
    "    df['State'] = state\n",
    "    df['Scenario'] = scenario\n",
    "\n",
    "# Inundation Scores\n",
    "for df, state, scenario in [\n",
    "    (df_inundation_scores_MA_585, 'MA', 'SSP585'), (df_inundation_scores_MA_585_new, 'MA', 'SSP585'),\n",
    "    (df_inundation_scores_MA_245, 'MA', 'SSP245'), (df_inundation_scores_MA_245_new, 'MA', 'SSP245'),\n",
    "    (df_inundation_scores_PA_585, 'PA', 'SSP585'), (df_inundation_scores_PA_585_new, 'PA', 'SSP585'),\n",
    "    (df_inundation_scores_PA_245, 'PA', 'SSP245'), (df_inundation_scores_PA_245_new, 'PA', 'SSP245'),\n",
    "    (df_inundation_scores_FL_585, 'FL', 'SSP585'), (df_inundation_scores_FL_585_new, 'FL', 'SSP585'),\n",
    "    (df_inundation_scores_FL_245, 'FL', 'SSP245'), (df_inundation_scores_FL_245_new, 'FL', 'SSP245'),\n",
    "    (df_inundation_scores_CA_585, 'CA', 'SSP585'), (df_inundation_scores_CA_585_new, 'CA', 'SSP585'),\n",
    "    (df_inundation_scores_CA_245, 'CA', 'SSP245'), (df_inundation_scores_CA_245_new, 'CA', 'SSP245'),\n",
    "    (df_inundation_scores_NY_585, 'NY', 'SSP585'), (df_inundation_scores_NY_245, 'NY', 'SSP245'),\n",
    "    (df_inundation_scores_BOS_585, 'BOS', 'SSP585'), (df_inundation_scores_BOS_585_new, 'BOS', 'SSP585'),\n",
    "    (df_inundation_scores_BOS_245, 'BOS', 'SSP245'), (df_inundation_scores_BOS_245_new, 'BOS', 'SSP245')\n",
    "]:\n",
    "    df['State'] = state\n",
    "    df['Scenario'] = scenario\n",
    "\n",
    "# Repeat the same process for Proximity, Storm, and Flood Scores\n",
    "# Proximity Scores\n",
    "for df, state, scenario in [\n",
    "    (df_proximity_scores_MA_585, 'MA', 'SSP585'), (df_proximity_scores_MA_585_new, 'MA', 'SSP585'),\n",
    "    (df_proximity_scores_MA_245, 'MA', 'SSP245'), (df_proximity_scores_MA_245_new, 'MA', 'SSP245'),\n",
    "    (df_proximity_scores_PA_585, 'PA', 'SSP585'), (df_proximity_scores_PA_585_new, 'PA', 'SSP585'),\n",
    "    (df_proximity_scores_PA_245, 'PA', 'SSP245'), (df_proximity_scores_PA_245_new, 'PA', 'SSP245'),\n",
    "    (df_proximity_scores_FL_585, 'FL', 'SSP585'), (df_proximity_scores_FL_585_new, 'FL', 'SSP585'),\n",
    "    (df_proximity_scores_FL_245, 'FL', 'SSP245'), (df_proximity_scores_FL_245_new, 'FL', 'SSP245'),\n",
    "    (df_proximity_scores_CA_585, 'CA', 'SSP585'), (df_proximity_scores_CA_585_new, 'CA', 'SSP585'),\n",
    "    (df_proximity_scores_CA_245, 'CA', 'SSP245'), (df_proximity_scores_CA_245_new, 'CA', 'SSP245'),\n",
    "    (df_proximity_scores_NY_585, 'NY', 'SSP585'), (df_proximity_scores_NY_245, 'NY', 'SSP245'),\n",
    "    (df_proximity_scores_BOS_585, 'BOS', 'SSP585'), (df_proximity_scores_BOS_585_new, 'BOS', 'SSP585'),\n",
    "    (df_proximity_scores_BOS_245, 'BOS', 'SSP245'), (df_proximity_scores_BOS_245_new, 'BOS', 'SSP245')\n",
    "]:\n",
    "    df['State'] = state\n",
    "    df['Scenario'] = scenario\n",
    "\n",
    "# Storm Scores\n",
    "for df, state, scenario in [\n",
    "    (df_storm_scores_MA_585, 'MA', 'SSP585'), (df_storm_scores_MA_585_new, 'MA', 'SSP585'),\n",
    "    (df_storm_scores_MA_245, 'MA', 'SSP245'), (df_storm_scores_MA_245_new, 'MA', 'SSP245'),\n",
    "    (df_storm_scores_PA_585, 'PA', 'SSP585'), (df_storm_scores_PA_585_new, 'PA', 'SSP585'),\n",
    "    (df_storm_scores_PA_245, 'PA', 'SSP245'), (df_storm_scores_PA_245_new, 'PA', 'SSP245'),\n",
    "    (df_storm_scores_FL_585, 'FL', 'SSP585'), (df_storm_scores_FL_585_new, 'FL', 'SSP585'),\n",
    "    (df_storm_scores_FL_245, 'FL', 'SSP245'), (df_storm_scores_FL_245_new, 'FL', 'SSP245'),\n",
    "    (df_storm_scores_CA_585, 'CA', 'SSP585'), (df_storm_scores_CA_585_new, 'CA', 'SSP585'),\n",
    "    (df_storm_scores_CA_245, 'CA', 'SSP245'), (df_storm_scores_CA_245_new, 'CA', 'SSP245'),\n",
    "    (df_storm_scores_NY_585, 'NY', 'SSP585'), (df_storm_scores_NY_245, 'NY', 'SSP245'),\n",
    "    (df_storm_scores_BOS_585, 'BOS', 'SSP585'), (df_storm_scores_BOS_585_new, 'BOS', 'SSP585'),\n",
    "    (df_storm_scores_BOS_245, 'BOS', 'SSP245'), (df_storm_scores_BOS_245_new, 'BOS', 'SSP245')\n",
    "]:\n",
    "    df['State'] = state\n",
    "    df['Scenario'] = scenario\n",
    "\n",
    "# Flood Scores\n",
    "for df, state, scenario in [\n",
    "    (df_flood_scores_MA_585, 'MA', 'SSP585'), (df_flood_scores_MA_585_new, 'MA', 'SSP585'),\n",
    "    (df_flood_scores_MA_245, 'MA', 'SSP245'), (df_flood_scores_MA_245_new, 'MA', 'SSP245'),\n",
    "    (df_flood_scores_PA_585, 'PA', 'SSP585'), (df_flood_scores_PA_585_new, 'PA', 'SSP585'),\n",
    "    (df_flood_scores_PA_245, 'PA', 'SSP245'), (df_flood_scores_PA_245_new, 'PA', 'SSP245'),\n",
    "    (df_flood_scores_FL_585, 'FL', 'SSP585'), (df_flood_scores_FL_585_new, 'FL', 'SSP585'),\n",
    "    (df_flood_scores_FL_245, 'FL', 'SSP245'), (df_flood_scores_FL_245_new, 'FL', 'SSP245'),\n",
    "    (df_flood_scores_CA_585, 'CA', 'SSP585'), (df_flood_scores_CA_585_new, 'CA', 'SSP585'),\n",
    "    (df_flood_scores_CA_245, 'CA', 'SSP245'), (df_flood_scores_CA_245_new, 'CA', 'SSP245'),\n",
    "    (df_flood_scores_NY_585, 'NY', 'SSP585'), (df_flood_scores_NY_245, 'NY', 'SSP245'),\n",
    "    (df_flood_scores_BOS_585, 'BOS', 'SSP585'), (df_flood_scores_BOS_585_new, 'BOS', 'SSP585'),\n",
    "    (df_flood_scores_BOS_245, 'BOS', 'SSP245'), (df_flood_scores_BOS_245_new, 'BOS', 'SSP245')\n",
    "]:\n",
    "    df['State'] = state\n",
    "    df['Scenario'] = scenario\n",
    "\n",
    "# Combine DataFrames for each score type\n",
    "\n",
    "# Year Scores\n",
    "combined_year_scores = pd.concat([\n",
    "    df_year_scores_MA_585, df_year_scores_MA_585_new, df_year_scores_MA_245, df_year_scores_MA_245_new,\n",
    "    df_year_scores_PA_585, df_year_scores_PA_585_new, df_year_scores_PA_245, df_year_scores_PA_245_new,\n",
    "    df_year_scores_FL_585, df_year_scores_FL_585_new, df_year_scores_FL_245, df_year_scores_FL_245_new,\n",
    "    df_year_scores_CA_585, df_year_scores_CA_585_new, df_year_scores_CA_245, df_year_scores_CA_245_new,\n",
    "    df_year_scores_NY_585, df_year_scores_NY_245,\n",
    "    df_year_scores_BOS_585, df_year_scores_BOS_585_new, df_year_scores_BOS_245, df_year_scores_BOS_245_new\n",
    "], ignore_index=True)\n",
    "\n",
    "# SLR Scores\n",
    "combined_slr_scores = pd.concat([\n",
    "    df_slr_scores_MA_585, df_slr_scores_MA_585_new, df_slr_scores_MA_245, df_slr_scores_MA_245_new,\n",
    "    df_slr_scores_PA_585, df_slr_scores_PA_585_new, df_slr_scores_PA_245, df_slr_scores_PA_245_new,\n",
    "    df_slr_scores_FL_585, df_slr_scores_FL_585_new, df_slr_scores_FL_245, df_slr_scores_FL_245_new,\n",
    "    df_slr_scores_CA_585, df_slr_scores_CA_585_new, df_slr_scores_CA_245, df_slr_scores_CA_245_new,\n",
    "    df_slr_scores_NY_585, df_slr_scores_NY_245,\n",
    "    df_slr_scores_BOS_585, df_slr_scores_BOS_585_new, df_slr_scores_BOS_245, df_slr_scores_BOS_245_new\n",
    "], ignore_index=True)\n",
    "\n",
    "# Inundation Scores\n",
    "combined_inundation_scores = pd.concat([\n",
    "    df_inundation_scores_MA_585, df_inundation_scores_MA_585_new, df_inundation_scores_MA_245, df_inundation_scores_MA_245_new,\n",
    "    df_inundation_scores_PA_585, df_inundation_scores_PA_585_new, df_inundation_scores_PA_245, df_inundation_scores_PA_245_new,\n",
    "    df_inundation_scores_FL_585, df_inundation_scores_FL_585_new, df_inundation_scores_FL_245, df_inundation_scores_FL_245_new,\n",
    "    df_inundation_scores_CA_585, df_inundation_scores_CA_585_new, df_inundation_scores_CA_245, df_inundation_scores_CA_245_new,\n",
    "    df_inundation_scores_NY_585, df_inundation_scores_NY_245,\n",
    "    df_inundation_scores_BOS_585, df_inundation_scores_BOS_585_new, df_inundation_scores_BOS_245, df_inundation_scores_BOS_245_new\n",
    "], ignore_index=True)\n",
    "\n",
    "# Proximity Scores\n",
    "combined_proximity_scores = pd.concat([\n",
    "    df_proximity_scores_MA_585, df_proximity_scores_MA_585_new, df_proximity_scores_MA_245, df_proximity_scores_MA_245_new,\n",
    "    df_proximity_scores_PA_585, df_proximity_scores_PA_585_new, df_proximity_scores_PA_245, df_proximity_scores_PA_245_new,\n",
    "    df_proximity_scores_FL_585, df_proximity_scores_FL_585_new, df_proximity_scores_FL_245, df_proximity_scores_FL_245_new,\n",
    "    df_proximity_scores_CA_585, df_proximity_scores_CA_585_new, df_proximity_scores_CA_245, df_proximity_scores_CA_245_new,\n",
    "    df_proximity_scores_NY_585, df_proximity_scores_NY_245,\n",
    "    df_proximity_scores_BOS_585, df_proximity_scores_BOS_585_new, df_proximity_scores_BOS_245, df_proximity_scores_BOS_245_new\n",
    "], ignore_index=True)\n",
    "\n",
    "# Storm Scores\n",
    "combined_storm_scores = pd.concat([\n",
    "    df_storm_scores_MA_585, df_storm_scores_MA_585_new, df_storm_scores_MA_245, df_storm_scores_MA_245_new,\n",
    "    df_storm_scores_PA_585, df_storm_scores_PA_585_new, df_storm_scores_PA_245, df_storm_scores_PA_245_new,\n",
    "    df_storm_scores_FL_585, df_storm_scores_FL_585_new, df_storm_scores_FL_245, df_storm_scores_FL_245_new,\n",
    "    df_storm_scores_CA_585, df_storm_scores_CA_585_new, df_storm_scores_CA_245, df_storm_scores_CA_245_new,\n",
    "    df_storm_scores_NY_585, df_storm_scores_NY_245,\n",
    "    df_storm_scores_BOS_585, df_storm_scores_BOS_585_new, df_storm_scores_BOS_245, df_storm_scores_BOS_245_new\n",
    "], ignore_index=True)\n",
    "\n",
    "# Flood Scores\n",
    "combined_flood_scores = pd.concat([\n",
    "    df_flood_scores_MA_585, df_flood_scores_MA_585_new, df_flood_scores_MA_245, df_flood_scores_MA_245_new,\n",
    "    df_flood_scores_PA_585, df_flood_scores_PA_585_new, df_flood_scores_PA_245, df_flood_scores_PA_245_new,\n",
    "    df_flood_scores_FL_585, df_flood_scores_FL_585_new, df_flood_scores_FL_245, df_flood_scores_FL_245_new,\n",
    "    df_flood_scores_CA_585, df_flood_scores_CA_585_new, df_flood_scores_CA_245, df_flood_scores_CA_245_new,\n",
    "    df_flood_scores_NY_585, df_flood_scores_NY_245,\n",
    "    df_flood_scores_BOS_585, df_flood_scores_BOS_585_new, df_flood_scores_BOS_245, df_flood_scores_BOS_245_new\n",
    "], ignore_index=True)\n",
    "\n",
    "# Export each score type to a separate sheet in an Excel workbook\n",
    "with pd.ExcelWriter('combined_scores_new.xlsx') as writer:\n",
    "    combined_year_scores.to_excel(writer, sheet_name='Year Scores', index=False)\n",
    "    combined_slr_scores.to_excel(writer, sheet_name='SLR Scores', index=False)\n",
    "    combined_inundation_scores.to_excel(writer, sheet_name='Inundation Scores', index=False)\n",
    "    combined_proximity_scores.to_excel(writer, sheet_name='Proximity Scores', index=False)\n",
    "    combined_storm_scores.to_excel(writer, sheet_name='Storm Scores', index=False)\n",
    "    combined_flood_scores.to_excel(writer, sheet_name='Flood Scores', index=False)\n",
    "\n",
    "print(\"Data exported to 'combined_scores_new.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847094e-387c-486e-9fd2-ee099490d0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
